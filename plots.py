# expr1
import matplotlib.pyplot as plt

steps = [8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 264, 272, 280, 288, 296, 304, 312, 320, 328, 336, 344, 352, 360, 368, 376, 384, 392, 400, 408, 416, 424, 432, 440, 448, 456, 464, 472, 480, 488, 496, 504, 512, 520, 528, 536, 544, 552, 560, 568, 576, 584, 592, 600, 608, 616, 624, 632, 640, 648, 656, 664, 672, 680, 688, 696, 704, 712, 720, 728, 736, 744, 752, 760, 768, 776, 784, 792, 800, 808, 816, 824, 832, 840, 848, 856, 864, 872, 880, 888, 896, 904, 912, 920, 928, 936, 944, 952, 960, 968, 976, 984, 992, 1000]
losses = [0.07196774333715439, 0.1397850513458252, 0.11878598729769389, 0.04086347669363022, 0.03427859246730804, 0.01900182416041692, 0.023246760879244124, 0.01956835761666298, 0.016342711117532518, 0.006159077957272529, 0.01203152130950581, 0.009535110866030058, 0.007400049039950738, 0.007334297788994653, 0.006318325797716777, 0.004528370685875416, 0.008777775308665107, 0.006254885345697403, 0.006508579771769674, 0.004261645302176475, 0.003756211627097357, 0.0042721260000358925, 0.004351143279801245, 0.004688107719024022, 0.0033803731203079224, 0.0033125888842802783, 0.0037701267887044836, 0.003308114196572985, 0.0030560796630793603, 0.0027429983019828796, 0.002790069387805077, 0.0031496998853981495, 0.00357813455841758, 0.0018175259670790505, 0.0029621524470193044, 0.0027462635189294815, 0.0028380536147066065, 0.0024380638803306377, 0.0021404618254074683, 0.0018972516059875487, 0.0020668641096208153, 0.0021687804588249753, 0.002135349567546401, 0.002188292585990646, 0.0017727580335405138, 0.0015821460148562555, 0.001851900777918227, 0.0017862141442795594, 0.0019119666243086056, 0.0017406710982322694, 0.0013559267801396986, 0.0015414675267843099, 0.001346501258184325, 0.0019431969633808843, 0.0016266904094002464, 0.0020876223487513407, 0.0015650300080316108, 0.0014597054185538456, 0.0015541943958250143, 0.0016488686203956604, 0.0013331196835783662, 0.0016764377153688861, 0.0016908260091902717, 0.0019282886059954762, 0.0013168803774393522, 0.0012779476290399377, 0.0015034335317896373, 0.0011224748895448796, 0.0012599059205124345, 0.0012167471860136305, 0.0010987557995487268, 0.0012617817976408536, 0.0013434446021302106, 0.0013505141880061175, 0.0012249983350435894, 0.0010791797386972529, 0.001153409094005436, 0.0010377496289901244, 0.0011487099566037141, 0.0011232010088860988, 0.0008552037639382445, 0.0009189835045395828, 0.0010682041925120067, 0.0010970487658466613, 0.0008317196193863364, 0.0008786102367001911, 0.0007413519524979865, 0.0007327001711184329, 0.0009534337212530415, 0.0007342901494767931, 0.0008593973713916736, 0.0008612126111984253, 0.000738232526727902, 0.000661787913834795, 0.0005337406145898919, 0.00055213268691053, 0.00043162672790055423, 0.0008266524848889331, 0.0006499556881008726, 0.00041834451258182526, 0.0006659355494055418, 0.0002593431329610301, 0.00024849608612870705, 0.00012747556544267214, 0.00014709922529402233, 0.0003433388148276311, 5.806606941830332e-05, 5.59358374664077e-05, 0.00012126216374406027, 5.44491477987983e-05, 0.0002661622255235105, 2.050942774595959e-05, 2.1576844320624275e-05, 0.00030605975342424293, 2.4482559250748677e-05, 2.057430462847496e-05, 0.00025069847320898983, 0.00013404033320435022, 1.3465715274840844e-05, 0.0002653473677734534, 0.00012546399808373334, 2.355712130055076e-05, 3.303944065076549e-05, 6.002674420033732e-05, 5.417536944150925e-05]
train_accuracies = [0.507, 0.511, 0.537, 0.538, 0.547, 0.523, 0.518, 0.52, 0.502, 0.5, 0.502, 0.491, 0.501, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.51, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.531, 0.512, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.528, 0.505, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.515, 0.521, 0.553, 0.581, 0.528, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.574, 0.52, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.522, 0.641, 0.636, 0.531, 0.521, 0.545, 0.704, 0.616, 0.526, 0.518, 0.512, 0.521, 0.55, 0.581, 0.594, 0.765, 0.859, 0.731, 0.751, 0.853, 0.886, 0.802, 0.698, 0.702, 0.798, 0.937, 0.888, 0.768, 0.643, 0.604, 0.696, 0.898, 0.966, 0.941, 0.94, 0.944, 0.942, 0.943, 0.949, 0.953, 0.956, 0.974, 0.972, 0.932, 0.871, 0.816, 0.988, 0.973, 0.938, 0.897, 0.913, 0.921, 0.961, 0.98, 0.991, 0.995, 0.993, 0.99]
test_accuracies = [0.49, 0.48, 0.49, 0.5, 0.49, 0.54, 0.56, 0.55, 0.51, 0.5, 0.5, 0.5, 0.5, 0.49, 0.5, 0.5, 0.5, 0.5, 0.49, 0.54, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.54, 0.51, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.53, 0.51, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.52, 0.54, 0.54, 0.61, 0.52, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.58, 0.53, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.51, 0.62, 0.63, 0.55, 0.53, 0.56, 0.68, 0.6, 0.51, 0.5, 0.5, 0.5, 0.56, 0.57, 0.58, 0.75, 0.83, 0.81, 0.82, 0.89, 0.86, 0.74, 0.64, 0.66, 0.74, 0.91, 0.89, 0.82, 0.67, 0.65, 0.71, 0.94, 0.96, 0.94, 0.93, 0.93, 0.94, 0.97, 0.97, 0.97, 0.97, 1.0, 0.98, 0.9, 0.86, 0.85, 0.98, 0.99, 0.95, 0.95, 0.95, 0.96, 0.97, 0.99, 1.0, 0.99, 0.99, 0.99]
wall_clock = [0.3023200035095215, 4.8990559577941895, 9.419800043106079, 14.016764640808105, 18.585764169692993, 23.18141269683838, 27.7666175365448, 32.35516929626465, 37.00760340690613, 41.61469864845276, 45.96810817718506, 50.54998421669006, 55.058135747909546, 59.5765175819397, 65.43414783477783, 70.00449395179749, 74.6309175491333, 79.24481225013733, 84.76367735862732, 89.4591383934021, 94.08054208755493, 98.70893955230713, 103.3635036945343, 108.00474429130554, 112.69246459007263, 117.40773606300354, 122.09359383583069, 126.5900022983551, 131.2111120223999, 135.8950915336609, 144.2973654270172, 148.98120713233948, 153.65685296058655, 158.30384755134583, 163.42573070526123, 168.06412434577942, 172.753901720047, 177.42062258720398, 182.04051327705383, 186.69923853874207, 191.34888553619385, 196.00659441947937, 200.67361521720886, 205.3376898765564, 209.9263048171997, 214.59425616264343, 219.32302355766296, 224.0269114971161, 228.72512817382812, 233.4212143421173, 238.03634357452393, 242.72293186187744, 247.33836245536804, 252.01188611984253, 256.69386100769043, 261.3426766395569, 266.02561259269714, 270.6996331214905, 275.3797199726105, 280.1694095134735, 284.77636075019836, 289.39984798431396, 294.19915652275085, 298.76448678970337, 303.3991422653198, 308.4318358898163, 313.1286606788635, 317.78119373321533, 322.46945095062256, 327.10877203941345, 331.7246491909027, 336.4200437068939, 341.11798310279846, 345.7251434326172, 350.4949564933777, 355.23113775253296, 359.91738986968994, 364.54142594337463, 369.20277404785156, 373.826890707016, 378.29331851005554, 382.9724588394165, 387.6136713027954, 392.30943965911865, 397.0121991634369, 401.69211745262146, 406.37210845947266, 411.0164396762848, 415.6295702457428, 420.3168308734894, 425.01072788238525, 429.6918263435364, 434.3827803134918, 439.03267431259155, 443.6955916881561, 448.3799273967743, 453.07870721817017, 457.7046489715576, 462.3009443283081, 466.89970326423645, 471.61621475219727, 476.2746708393097, 481.00675201416016, 485.6566526889801, 490.3159489631653, 494.90990257263184, 499.62427496910095, 504.27613735198975, 508.9601516723633, 513.6884713172913, 518.3864889144897, 523.0225365161896, 527.7105762958527, 532.3724219799042, 537.0308916568756, 541.6972966194153, 546.3040318489075, 550.9908757209778, 555.7198612689972, 560.3891491889954, 564.9916565418243, 569.6701216697693, 574.3742372989655, 578.9738125801086, 583.6094317436218]


plt.subplot(2,2,1)
plt.title(f'Train Accuracy vs. seen examples')
plt.plot(steps, train_accuracies)
plt.xlabel(f'seen words')
plt.ylabel(f'Train accuracy')

plt.subplot(2,2,2)
plt.title(f'Test Accuracy vs. seen examples')
plt.plot(steps, test_accuracies)
plt.xlabel(f'seen words')
plt.ylabel(f'Test accuracy')

plt.subplot(2,2,3)
plt.title(f'Train loss vs. seen examples')
plt.plot(steps, losses)
plt.xlabel(f'seen words')
plt.ylabel(f'loss')

plt.subplot(2,2,4)
plt.title(f'Seconds vs. seen examples')
plt.plot(steps, wall_clock)
plt.xlabel(f'seen words')
plt.ylabel(f'seconds')

plt.show()

