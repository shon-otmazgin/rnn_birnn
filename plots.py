# expr1
import matplotlib.pyplot as plt

steps = [8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 264, 272, 280, 288, 296, 304, 312, 320, 328, 336, 344, 352, 360, 368, 376, 384, 392, 400, 408, 416, 424, 432, 440, 448, 456, 464, 472, 480, 488, 496, 504, 512, 520, 528, 536, 544, 552, 560, 568, 576, 584, 592, 600, 608, 616, 624, 632, 640, 648, 656, 664, 672, 680, 688, 696, 704, 712, 720, 728, 736, 744, 752, 760, 768, 776, 784, 792, 800, 808, 816, 824, 832, 840, 848, 856, 864, 872, 880, 888, 896, 904, 912, 920, 928, 936, 944, 952, 960, 968, 976, 984, 992, 1000]
losses = [0.09958566725254059, 0.3023950755596161, 0.07564851144949596, 0.016705522313714027, 0.023050077259540558, 0.04477802415688833, 0.017960028988974436, 0.015931133180856705, 0.012938110364807976, 0.007799683511257172, 0.013034685091538862, 0.011762067675590515, 0.008593347210150499, 0.010249614715576172, 0.008458991845448812, 0.008206721395254135, 0.005124549217083875, 0.0049399468633863665, 0.005782631667036759, 0.0042334221303462986, 0.004940209289391835, 0.00347328321500258, 0.0025626165063484855, 0.0037929797545075417, 0.002301621288061142, 0.005248713951844435, 0.0038921402560340036, 0.003686893199171339, 0.002988031712071649, 0.002730988711118698, 0.004364777957239459, 0.0021289975848048925, 0.0029009253238186693, 0.002821619019788854, 0.0024331844278744287, 0.0028899458961354364, 0.002447056206496986, 0.002154872409607235, 0.002801780899365743, 0.0022204266861081123, 0.002071601588551591, 0.0025033833725111826, 0.002288667961608532, 0.001927658746188337, 0.0013959990607367622, 0.0021768533017324366, 0.0015298622719785, 0.0018617476647098858, 0.0020930316983437053, 0.0017608551681041718, 0.0018772937211335875, 0.001661197210733707, 0.001635116366845257, 0.0015904732324458935, 0.001408564502542669, 0.0015147074258753232, 0.0018780009265531574, 0.0014278784651180793, 0.0015117399773355257, 0.0014078696568806967, 0.0014545541317736517, 0.0013487764423893345, 0.0012193320999069818, 0.001437400933355093, 0.0012366074782151442, 0.001398658887906508, 0.0014800904179686932, 0.0012556565377642127, 0.0011749439265417016, 0.0011289972279753004, 0.0012067789343041433, 0.001248507450024287, 0.001116596672632923, 0.0010741890684978382, 0.0010879737138748168, 0.000996571034193039, 0.0010305659530998824, 0.0010446107540375148, 0.0009497157946417603, 0.0009291732683777809, 0.0010572557831987923, 0.0009048011426518603, 0.0008185100124542972, 0.0008564311124029613, 0.000684878317748799, 0.0006624176058658334, 0.0007434627790560668, 0.0006198282776908441, 0.0010404894860942713, 0.00042459712260299256, 0.0004895429362307538, 0.0006463953658290532, 0.0006211362378571623, 0.0004387238320518047, 0.0005381206932820772, 0.001084200261781613, 0.0004661779505075868, 0.0003416712338827094, 0.00024193019201659193, 0.0002496888116002083, 0.0002447791987716561, 0.00047790291993057024, 0.00018273210785921338, 0.000246168293345433, 4.608292310010819e-05, 0.000458424559460496, 4.104734180825893e-05, 2.9855567306556083e-05, 6.208152706743381e-05, 4.945067604157058e-05, 3.0434333828386958e-05, 6.325752474367619e-05, 3.251265858061018e-05, 9.794119059255249e-06, 1.8378788524347802e-05, 7.1687175258030664e-06, 6.477526114441645e-06, 8.064478545023476e-06, 5.3012788593142975e-06, 6.101703426490227e-06, 5.5323456883553634e-06, 3.33564487652334e-06, 5.540180557627019e-06, 2.672216778005203e-06, 5.209745839238166e-06]
train_accuracies = [0.499, 0.499, 0.516, 0.489, 0.505, 0.536, 0.513, 0.514, 0.536, 0.516, 0.513, 0.52, 0.548, 0.514, 0.517, 0.515, 0.519, 0.543, 0.508, 0.505, 0.505, 0.506, 0.502, 0.501, 0.5, 0.5, 0.501, 0.512, 0.499, 0.5, 0.5, 0.5, 0.5, 0.499, 0.499, 0.537, 0.517, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.503, 0.542, 0.539, 0.519, 0.519, 0.527, 0.568, 0.577, 0.545, 0.501, 0.501, 0.5, 0.5, 0.5, 0.503, 0.554, 0.66, 0.635, 0.577, 0.586, 0.636, 0.711, 0.538, 0.502, 0.501, 0.502, 0.51, 0.649, 0.662, 0.729, 0.806, 0.834, 0.805, 0.76, 0.886, 0.933, 0.887, 0.955, 0.959, 0.861, 0.805, 0.803, 0.949, 0.956, 0.872, 0.879, 0.962, 0.793, 0.752, 0.799, 0.898, 0.946, 0.929, 0.899, 0.905, 0.935, 0.968, 0.988, 0.986, 0.989, 0.991, 0.992, 0.993, 0.995, 0.994, 0.996, 0.997, 0.997, 0.999, 0.999, 0.999, 0.997, 0.995, 0.993, 0.992, 0.99, 0.988, 0.988, 0.985]
test_accuracies = [0.5, 0.43, 0.5, 0.51, 0.5, 0.52, 0.46, 0.45, 0.5, 0.48, 0.48, 0.46, 0.56, 0.55, 0.53, 0.52, 0.5, 0.52, 0.53, 0.48, 0.49, 0.5, 0.5, 0.5, 0.5, 0.5, 0.53, 0.49, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.52, 0.52, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.52, 0.51, 0.47, 0.52, 0.52, 0.53, 0.56, 0.55, 0.54, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.55, 0.6, 0.61, 0.54, 0.55, 0.59, 0.69, 0.54, 0.5, 0.5, 0.5, 0.52, 0.64, 0.63, 0.73, 0.81, 0.82, 0.81, 0.73, 0.89, 0.97, 0.97, 0.97, 0.94, 0.87, 0.77, 0.78, 0.94, 0.96, 0.94, 0.95, 0.96, 0.75, 0.7, 0.76, 0.85, 0.93, 0.91, 0.87, 0.88, 0.93, 0.97, 0.98, 0.97, 0.97, 0.97, 0.97, 0.99, 0.99, 0.99, 0.99, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99]
wall_clock = [0.3134768009185791, 2.3643884658813477, 4.3985207080841064, 6.380804777145386, 8.456536531448364, 10.491454839706421, 12.519589185714722, 14.727746486663818, 16.80622148513794, 18.863658666610718, 20.910950899124146, 23.69485569000244, 25.75455904006958, 27.715378284454346, 29.75999402999878, 31.92124843597412, 33.92634630203247, 35.94925665855408, 37.9856595993042, 40.0272376537323, 42.08686876296997, 44.115172386169434, 46.120914697647095, 48.189549684524536, 50.13899374008179, 52.14023947715759, 54.15384650230408, 56.18661975860596, 58.19270348548889, 60.210370779037476, 62.22905874252319, 64.33824753761292, 66.36348700523376, 68.40539598464966, 70.48441958427429, 72.50787591934204, 74.6294014453888, 76.61081790924072, 78.67249369621277, 80.70668244361877, 82.76974606513977, 84.7715892791748, 86.79920244216919, 88.82396125793457, 90.87205862998962, 92.93200278282166, 95.03252124786377, 97.08914637565613, 99.22076988220215, 101.24087262153625, 103.24938416481018, 105.24573540687561, 107.29796886444092, 109.28791093826294, 111.23853516578674, 113.13745546340942, 115.18113803863525, 117.1558096408844, 119.20827579498291, 121.18057465553284, 123.32777237892151, 125.32618045806885, 127.33254909515381, 129.3500473499298, 131.385338306427, 133.42734122276306, 135.437180519104, 137.45515537261963, 139.43601298332214, 141.44800329208374, 143.44831156730652, 145.45106887817383, 147.50026082992554, 149.50181412696838, 151.512770652771, 153.55743956565857, 155.65709972381592, 157.69969201087952, 159.71840524673462, 161.73559069633484, 163.78866505622864, 165.74433755874634, 167.71679520606995, 169.815815448761, 171.88157105445862, 173.83473205566406, 175.914076089859, 177.9774649143219, 180.54798889160156, 182.5468635559082, 184.56836557388306, 186.64894938468933, 188.66993403434753, 190.74844026565552, 192.764493227005, 194.87768054008484, 196.9314830303192, 198.93442797660828, 200.99905276298523, 203.08103895187378, 205.11690187454224, 207.10137939453125, 209.15955185890198, 211.26615953445435, 213.33206701278687, 215.34664726257324, 217.372985124588, 219.39410638809204, 221.25626754760742, 223.29900693893433, 225.40201616287231, 227.4362142086029, 229.47213864326477, 231.5518364906311, 233.56026411056519, 235.6309096813202, 237.64663243293762, 239.6794638633728, 241.7113959789276, 243.87507390975952, 245.9127767086029, 247.88690495491028, 249.88643789291382, 251.98339176177979, 254.0079414844513]

plt.subplot(2,2,1)
plt.title(f'Train Accuracy vs. seen examples')
plt.plot(steps, train_accuracies)
plt.xlabel(f'seen words')
plt.ylabel(f'Train accuracy')

plt.subplot(2,2,2)
plt.title(f'Test Accuracy vs. seen examples')
plt.plot(steps, test_accuracies)
plt.xlabel(f'seen words')
plt.ylabel(f'Test accuracy')

plt.subplot(2,2,3)
plt.title(f'Train loss vs. seen examples')
plt.plot(steps, losses)
plt.xlabel(f'seen words')
plt.ylabel(f'loss')

plt.subplot(2,2,4)
plt.title(f'Seconds vs. seen examples')
plt.plot(steps, wall_clock)
plt.xlabel(f'seen words')
plt.ylabel(f'seconds')

plt.show()

